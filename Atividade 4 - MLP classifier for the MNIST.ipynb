{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PC1_Ativ4_MLP_MNIST.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **Notebook PC1_Ativ4**\n","## MLP classifier for the MNIST database.\n","**Professor:** Fernando J. Von Zuben <br>\n","**Aluno(a):** Caio Francisco Garcia de Lima **RA** 195210\n"],"metadata":{"id":"xMsNyA3WPrER"}},{"cell_type":"markdown","source":["**A sua proposta deve ser capaz de superar o desempenho desta sugestão e você deve descrever de forma objetiva o caminho trilhado até a sua configuração final de código para a rede neural MLP.**\n","\n","<p align=\"justify\">Para melhorar o desempenho da rede utilizando o dataset minist optei por testar diversas configurações por conta própria, originalmente a rede obteve um desempenho de 0.9788, fui capaz de superar esse desempenho (0.9855) alterando o dropout para 40% e a quantidade de épocas para 200, essa base não é tão desafiadora para o classificador quanto a CIFAR-10.\n","De maneira semelhante testei diversas configurações de hiper parâmetros para a CIFAR-10, mas todas geraram poucas diferenças no resultado final, para melhorar o resultado conferi outras abordagens e optei por aumentar o número de camadas e de épocas(0.4558). O valor original para o CIFAR-10 era 0.4078\n"," </p>"],"metadata":{"id":"_IPTMZuWTk3i"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"feKIgQlnaT7U","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1652109411358,"user_tz":180,"elapsed":999685,"user":{"displayName":"Caio Lima","userId":"03882685747773195708"}},"outputId":"44116a00-7dca-4793-fd59-dba23abd2460"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n","Epoch 1/100\n","1875/1875 [==============================] - 12s 6ms/step - loss: 0.2475 - accuracy: 0.9271\n","Epoch 2/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.1210 - accuracy: 0.9630\n","Epoch 3/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0931 - accuracy: 0.9707\n","Epoch 4/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0767 - accuracy: 0.9759\n","Epoch 5/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0636 - accuracy: 0.9797\n","Epoch 6/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0590 - accuracy: 0.9809\n","Epoch 7/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0507 - accuracy: 0.9840\n","Epoch 8/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0460 - accuracy: 0.9844\n","Epoch 9/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0444 - accuracy: 0.9851\n","Epoch 10/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0392 - accuracy: 0.9868\n","Epoch 11/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0382 - accuracy: 0.9872\n","Epoch 12/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0337 - accuracy: 0.9890\n","Epoch 13/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0329 - accuracy: 0.9890\n","Epoch 14/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0333 - accuracy: 0.9893\n","Epoch 15/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0293 - accuracy: 0.9901\n","Epoch 16/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0304 - accuracy: 0.9904\n","Epoch 17/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0265 - accuracy: 0.9913\n","Epoch 18/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0289 - accuracy: 0.9907\n","Epoch 19/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0281 - accuracy: 0.9907\n","Epoch 20/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0250 - accuracy: 0.9919\n","Epoch 21/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0266 - accuracy: 0.9916\n","Epoch 22/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0243 - accuracy: 0.9919\n","Epoch 23/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0230 - accuracy: 0.9926\n","Epoch 24/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0274 - accuracy: 0.9913\n","Epoch 25/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0217 - accuracy: 0.9927\n","Epoch 26/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0219 - accuracy: 0.9928\n","Epoch 27/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0223 - accuracy: 0.9925\n","Epoch 28/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0207 - accuracy: 0.9936\n","Epoch 29/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0205 - accuracy: 0.9934\n","Epoch 30/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0215 - accuracy: 0.9935\n","Epoch 31/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0208 - accuracy: 0.9938\n","Epoch 32/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0214 - accuracy: 0.9936\n","Epoch 33/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0197 - accuracy: 0.9938\n","Epoch 34/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0203 - accuracy: 0.9936\n","Epoch 35/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0180 - accuracy: 0.9941\n","Epoch 36/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0182 - accuracy: 0.9948\n","Epoch 37/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0208 - accuracy: 0.9939\n","Epoch 38/100\n","1875/1875 [==============================] - 10s 6ms/step - loss: 0.0166 - accuracy: 0.9947\n","Epoch 39/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0180 - accuracy: 0.9945\n","Epoch 40/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0192 - accuracy: 0.9945\n","Epoch 41/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0174 - accuracy: 0.9947\n","Epoch 42/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0194 - accuracy: 0.9941\n","Epoch 43/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0157 - accuracy: 0.9952\n","Epoch 44/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0197 - accuracy: 0.9945\n","Epoch 45/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0184 - accuracy: 0.9947\n","Epoch 46/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0164 - accuracy: 0.9952\n","Epoch 47/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0189 - accuracy: 0.9946\n","Epoch 48/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0169 - accuracy: 0.9952\n","Epoch 49/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0186 - accuracy: 0.9949\n","Epoch 50/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0171 - accuracy: 0.9951\n","Epoch 51/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0194 - accuracy: 0.9944\n","Epoch 52/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0164 - accuracy: 0.9957\n","Epoch 53/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0174 - accuracy: 0.9954\n","Epoch 54/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0143 - accuracy: 0.9959\n","Epoch 55/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0160 - accuracy: 0.9954\n","Epoch 56/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - accuracy: 0.9952\n","Epoch 57/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0177 - accuracy: 0.9953\n","Epoch 58/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0183 - accuracy: 0.9950\n","Epoch 59/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0125 - accuracy: 0.9961\n","Epoch 60/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0149 - accuracy: 0.9958\n","Epoch 61/100\n","1875/1875 [==============================] - 10s 6ms/step - loss: 0.0175 - accuracy: 0.9951\n","Epoch 62/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0138 - accuracy: 0.9962\n","Epoch 63/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0173 - accuracy: 0.9956\n","Epoch 64/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0157 - accuracy: 0.9961\n","Epoch 65/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0179 - accuracy: 0.9956\n","Epoch 66/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0150 - accuracy: 0.9959\n","Epoch 67/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0141 - accuracy: 0.9957\n","Epoch 68/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0148 - accuracy: 0.9963\n","Epoch 69/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0168 - accuracy: 0.9956\n","Epoch 70/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0162 - accuracy: 0.9959\n","Epoch 71/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0166 - accuracy: 0.9959\n","Epoch 72/100\n","1875/1875 [==============================] - 9s 5ms/step - loss: 0.0137 - accuracy: 0.9962\n","Epoch 73/100\n","1875/1875 [==============================] - 10s 6ms/step - loss: 0.0148 - accuracy: 0.9962\n","Epoch 74/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0173 - accuracy: 0.9956\n","Epoch 75/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0142 - accuracy: 0.9961\n","Epoch 76/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0163 - accuracy: 0.9962\n","Epoch 77/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0141 - accuracy: 0.9962\n","Epoch 78/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0133 - accuracy: 0.9968\n","Epoch 79/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0147 - accuracy: 0.9963\n","Epoch 80/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0140 - accuracy: 0.9963\n","Epoch 81/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0148 - accuracy: 0.9964\n","Epoch 82/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0129 - accuracy: 0.9966\n","Epoch 83/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0151 - accuracy: 0.9964\n","Epoch 84/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0133 - accuracy: 0.9965\n","Epoch 85/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0137 - accuracy: 0.9965\n","Epoch 86/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0129 - accuracy: 0.9970\n","Epoch 87/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0175 - accuracy: 0.9960\n","Epoch 88/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0110 - accuracy: 0.9970\n","Epoch 89/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0127 - accuracy: 0.9967\n","Epoch 90/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0157 - accuracy: 0.9962\n","Epoch 91/100\n","1875/1875 [==============================] - 12s 6ms/step - loss: 0.0158 - accuracy: 0.9962\n","Epoch 92/100\n","1875/1875 [==============================] - 10s 6ms/step - loss: 0.0123 - accuracy: 0.9973\n","Epoch 93/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0131 - accuracy: 0.9968\n","Epoch 94/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0154 - accuracy: 0.9966\n","Epoch 95/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0156 - accuracy: 0.9963\n","Epoch 96/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0153 - accuracy: 0.9966\n","Epoch 97/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0149 - accuracy: 0.9965\n","Epoch 98/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0134 - accuracy: 0.9968\n","Epoch 99/100\n","1875/1875 [==============================] - 11s 6ms/step - loss: 0.0139 - accuracy: 0.9969\n","Epoch 100/100\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0143 - accuracy: 0.9966\n","Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten_2 (Flatten)         (32, 784)                 0         \n","                                                                 \n"," dense_8 (Dense)             (32, 512)                 401920    \n","                                                                 \n"," dropout_6 (Dropout)         (32, 512)                 0         \n","                                                                 \n"," dense_9 (Dense)             (32, 10)                  5130      \n","                                                                 \n","=================================================================\n","Total params: 407,050\n","Trainable params: 407,050\n","Non-trainable params: 0\n","_________________________________________________________________\n","Evaluate on test data\n","313/313 [==============================] - 1s 4ms/step - loss: 0.1723 - accuracy: 0.9855\n","test loss, test acc: [0.17233221232891083, 0.9854999780654907]\n","Model saved to disk\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["import tensorflow as tf\n","import os\n","mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train),(x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n","  tf.keras.layers.Dropout(0.4),\n","  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n","])\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","model.fit(x_train, y_train, epochs=100)\n","model.summary()\n","# Evaluate the model on the test data using `evaluate`\n","print(\"Evaluate on test data\")\n","results = model.evaluate(x_test, y_test)\n","print(\"test loss, test acc:\", results)\n","model_json = model.to_json()\n","json_file = open(\"model_MLP.json\", \"w\")\n","json_file.write(model_json)\n","json_file.close()\n","model.save_weights(\"model_MLP.h5\")\n","print(\"Model saved to disk\")\n","os.getcwd()"]},{"cell_type":"code","source":["import tensorflow as tf\n","import os\n","cifar10 = tf.keras.datasets.cifar10\n","\n","(x_train, y_train),(x_test, y_test) = cifar10.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n","  tf.keras.layers.Dropout(0.2),\n","  \n","  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n","  tf.keras.layers.Dropout(0.2),\n","  \n","  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n","  tf.keras.layers.Dropout(0.2),\n","\n","  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n","])\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","model.fit(x_train, y_train, epochs=200)\n","model.summary()\n","# Evaluate the model on the test data using `evaluate`\n","print(\"Evaluate on test data\")\n","results = model.evaluate(x_test, y_test)\n","print(\"test loss, test acc:\", results)\n","model_json = model.to_json()\n","json_file = open(\"model_MLP.json\", \"w\")\n","json_file.write(model_json)\n","json_file.close()\n","model.save_weights(\"model_MLP.h5\")\n","print(\"Model saved to disk\")\n","os.getcwd()"],"metadata":{"id":"jMGREkVU7VWn","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1652121634357,"user_tz":180,"elapsed":11852237,"user":{"displayName":"Caio Lima","userId":"03882685747773195708"}},"outputId":"1dfcfab8-2a48-486c-a5af-46f1e309b8c7"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.9960 - accuracy: 0.2641\n","Epoch 2/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.8660 - accuracy: 0.3160\n","Epoch 3/200\n","1563/1563 [==============================] - 55s 35ms/step - loss: 1.8206 - accuracy: 0.3373\n","Epoch 4/200\n","1563/1563 [==============================] - 57s 36ms/step - loss: 1.7896 - accuracy: 0.3486\n","Epoch 5/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.7665 - accuracy: 0.3608\n","Epoch 6/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.7434 - accuracy: 0.3694\n","Epoch 7/200\n","1563/1563 [==============================] - 56s 36ms/step - loss: 1.7363 - accuracy: 0.3669\n","Epoch 8/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.7258 - accuracy: 0.3751\n","Epoch 9/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.7171 - accuracy: 0.3750\n","Epoch 10/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.7064 - accuracy: 0.3852\n","Epoch 11/200\n","1563/1563 [==============================] - 57s 37ms/step - loss: 1.7090 - accuracy: 0.3803\n","Epoch 12/200\n","1563/1563 [==============================] - 56s 36ms/step - loss: 1.6931 - accuracy: 0.3905\n","Epoch 13/200\n","1563/1563 [==============================] - 56s 36ms/step - loss: 1.6850 - accuracy: 0.3910\n","Epoch 14/200\n","1563/1563 [==============================] - 57s 37ms/step - loss: 1.6839 - accuracy: 0.3956\n","Epoch 15/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.6843 - accuracy: 0.3907\n","Epoch 16/200\n","1563/1563 [==============================] - 59s 37ms/step - loss: 1.6759 - accuracy: 0.3957\n","Epoch 17/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.6766 - accuracy: 0.3940\n","Epoch 18/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.6683 - accuracy: 0.3972\n","Epoch 19/200\n","1563/1563 [==============================] - 57s 37ms/step - loss: 1.6629 - accuracy: 0.4000\n","Epoch 20/200\n","1563/1563 [==============================] - 57s 36ms/step - loss: 1.6584 - accuracy: 0.4036\n","Epoch 21/200\n","1563/1563 [==============================] - 57s 36ms/step - loss: 1.6564 - accuracy: 0.4021\n","Epoch 22/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.6510 - accuracy: 0.4033\n","Epoch 23/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.6526 - accuracy: 0.4007\n","Epoch 24/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.6473 - accuracy: 0.4030\n","Epoch 25/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.6457 - accuracy: 0.4071\n","Epoch 26/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.6429 - accuracy: 0.4070\n","Epoch 27/200\n","1563/1563 [==============================] - 57s 37ms/step - loss: 1.6351 - accuracy: 0.4102\n","Epoch 28/200\n","1563/1563 [==============================] - 56s 36ms/step - loss: 1.6432 - accuracy: 0.4086\n","Epoch 29/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.6301 - accuracy: 0.4120\n","Epoch 30/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.6380 - accuracy: 0.4113\n","Epoch 31/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.6360 - accuracy: 0.4108\n","Epoch 32/200\n","1563/1563 [==============================] - 60s 39ms/step - loss: 1.6288 - accuracy: 0.4124\n","Epoch 33/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.6314 - accuracy: 0.4133\n","Epoch 34/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.6327 - accuracy: 0.4120\n","Epoch 35/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.6276 - accuracy: 0.4129\n","Epoch 36/200\n","1563/1563 [==============================] - 57s 37ms/step - loss: 1.6257 - accuracy: 0.4147\n","Epoch 37/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.6273 - accuracy: 0.4141\n","Epoch 38/200\n","1563/1563 [==============================] - 60s 39ms/step - loss: 1.6253 - accuracy: 0.4134\n","Epoch 39/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.6214 - accuracy: 0.4172\n","Epoch 40/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.6175 - accuracy: 0.4150\n","Epoch 41/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.6125 - accuracy: 0.4198\n","Epoch 42/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.6157 - accuracy: 0.4181\n","Epoch 43/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.6126 - accuracy: 0.4179\n","Epoch 44/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.6127 - accuracy: 0.4190\n","Epoch 45/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.6130 - accuracy: 0.4196\n","Epoch 46/200\n","1563/1563 [==============================] - 57s 37ms/step - loss: 1.6111 - accuracy: 0.4214\n","Epoch 47/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.6057 - accuracy: 0.4228\n","Epoch 48/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.6050 - accuracy: 0.4224\n","Epoch 49/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.6070 - accuracy: 0.4197\n","Epoch 50/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.6033 - accuracy: 0.4216\n","Epoch 51/200\n","1563/1563 [==============================] - 60s 39ms/step - loss: 1.5994 - accuracy: 0.4234\n","Epoch 52/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.6016 - accuracy: 0.4254\n","Epoch 53/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5981 - accuracy: 0.4278\n","Epoch 54/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.6010 - accuracy: 0.4217\n","Epoch 55/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5988 - accuracy: 0.4238\n","Epoch 56/200\n","1563/1563 [==============================] - 62s 40ms/step - loss: 1.5951 - accuracy: 0.4266\n","Epoch 57/200\n","1563/1563 [==============================] - 57s 37ms/step - loss: 1.5877 - accuracy: 0.4287\n","Epoch 58/200\n","1563/1563 [==============================] - 57s 37ms/step - loss: 1.6096 - accuracy: 0.4219\n","Epoch 59/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.6036 - accuracy: 0.4267\n","Epoch 60/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5998 - accuracy: 0.4255\n","Epoch 61/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.6002 - accuracy: 0.4289\n","Epoch 62/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.6034 - accuracy: 0.4237\n","Epoch 63/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.6020 - accuracy: 0.4249\n","Epoch 64/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.6011 - accuracy: 0.4257\n","Epoch 65/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.6003 - accuracy: 0.4262\n","Epoch 66/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5979 - accuracy: 0.4247\n","Epoch 67/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5973 - accuracy: 0.4234\n","Epoch 68/200\n","1563/1563 [==============================] - 56s 36ms/step - loss: 1.5921 - accuracy: 0.4283\n","Epoch 69/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5878 - accuracy: 0.4293\n","Epoch 70/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5904 - accuracy: 0.4279\n","Epoch 71/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5897 - accuracy: 0.4292\n","Epoch 72/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5888 - accuracy: 0.4300\n","Epoch 73/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5898 - accuracy: 0.4315\n","Epoch 74/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5827 - accuracy: 0.4349\n","Epoch 75/200\n","1563/1563 [==============================] - 57s 37ms/step - loss: 1.5824 - accuracy: 0.4335\n","Epoch 76/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5944 - accuracy: 0.4272\n","Epoch 77/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5861 - accuracy: 0.4311\n","Epoch 78/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5841 - accuracy: 0.4309\n","Epoch 79/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5885 - accuracy: 0.4304\n","Epoch 80/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5772 - accuracy: 0.4345\n","Epoch 81/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5808 - accuracy: 0.4321\n","Epoch 82/200\n","1563/1563 [==============================] - 56s 36ms/step - loss: 1.5798 - accuracy: 0.4347\n","Epoch 83/200\n","1563/1563 [==============================] - 57s 36ms/step - loss: 1.5831 - accuracy: 0.4306\n","Epoch 84/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5752 - accuracy: 0.4353\n","Epoch 85/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5762 - accuracy: 0.4348\n","Epoch 86/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5720 - accuracy: 0.4375\n","Epoch 87/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5757 - accuracy: 0.4345\n","Epoch 88/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5729 - accuracy: 0.4347\n","Epoch 89/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5750 - accuracy: 0.4338\n","Epoch 90/200\n","1563/1563 [==============================] - 57s 37ms/step - loss: 1.5765 - accuracy: 0.4353\n","Epoch 91/200\n","1563/1563 [==============================] - 59s 37ms/step - loss: 1.5717 - accuracy: 0.4371\n","Epoch 92/200\n","1563/1563 [==============================] - 62s 40ms/step - loss: 1.5663 - accuracy: 0.4402\n","Epoch 93/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5669 - accuracy: 0.4364\n","Epoch 94/200\n","1563/1563 [==============================] - 57s 37ms/step - loss: 1.5688 - accuracy: 0.4363\n","Epoch 95/200\n","1563/1563 [==============================] - 57s 37ms/step - loss: 1.5745 - accuracy: 0.4345\n","Epoch 96/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5637 - accuracy: 0.4389\n","Epoch 97/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5682 - accuracy: 0.4383\n","Epoch 98/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5576 - accuracy: 0.4411\n","Epoch 99/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5694 - accuracy: 0.4373\n","Epoch 100/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5653 - accuracy: 0.4377\n","Epoch 101/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5694 - accuracy: 0.4376\n","Epoch 102/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5774 - accuracy: 0.4335\n","Epoch 103/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5860 - accuracy: 0.4338\n","Epoch 104/200\n","1563/1563 [==============================] - 57s 37ms/step - loss: 1.5800 - accuracy: 0.4324\n","Epoch 105/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5773 - accuracy: 0.4375\n","Epoch 106/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5789 - accuracy: 0.4332\n","Epoch 107/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5764 - accuracy: 0.4313\n","Epoch 108/200\n","1563/1563 [==============================] - 55s 35ms/step - loss: 1.5766 - accuracy: 0.4352\n","Epoch 109/200\n","1563/1563 [==============================] - 62s 39ms/step - loss: 1.5755 - accuracy: 0.4351\n","Epoch 110/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5771 - accuracy: 0.4351\n","Epoch 111/200\n","1563/1563 [==============================] - 62s 40ms/step - loss: 1.5748 - accuracy: 0.4367\n","Epoch 112/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5748 - accuracy: 0.4363\n","Epoch 113/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5743 - accuracy: 0.4341\n","Epoch 114/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5692 - accuracy: 0.4377\n","Epoch 115/200\n","1563/1563 [==============================] - 57s 36ms/step - loss: 1.5662 - accuracy: 0.4390\n","Epoch 116/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5675 - accuracy: 0.4367\n","Epoch 117/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5734 - accuracy: 0.4371\n","Epoch 118/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5713 - accuracy: 0.4365\n","Epoch 119/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5651 - accuracy: 0.4406\n","Epoch 120/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5655 - accuracy: 0.4407\n","Epoch 121/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5613 - accuracy: 0.4415\n","Epoch 122/200\n","1563/1563 [==============================] - 60s 39ms/step - loss: 1.5608 - accuracy: 0.4412\n","Epoch 123/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5609 - accuracy: 0.4402\n","Epoch 124/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5563 - accuracy: 0.4421\n","Epoch 125/200\n","1563/1563 [==============================] - 59s 37ms/step - loss: 1.5679 - accuracy: 0.4393\n","Epoch 126/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5636 - accuracy: 0.4420\n","Epoch 127/200\n","1563/1563 [==============================] - 63s 40ms/step - loss: 1.5571 - accuracy: 0.4417\n","Epoch 128/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5553 - accuracy: 0.4420\n","Epoch 129/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5584 - accuracy: 0.4427\n","Epoch 130/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5552 - accuracy: 0.4448\n","Epoch 131/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5562 - accuracy: 0.4412\n","Epoch 132/200\n","1563/1563 [==============================] - 57s 37ms/step - loss: 1.5523 - accuracy: 0.4426\n","Epoch 133/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5531 - accuracy: 0.4421\n","Epoch 134/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5530 - accuracy: 0.4463\n","Epoch 135/200\n","1563/1563 [==============================] - 57s 37ms/step - loss: 1.5639 - accuracy: 0.4397\n","Epoch 136/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5538 - accuracy: 0.4419\n","Epoch 137/200\n","1563/1563 [==============================] - 60s 39ms/step - loss: 1.5562 - accuracy: 0.4445\n","Epoch 138/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5493 - accuracy: 0.4469\n","Epoch 139/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5513 - accuracy: 0.4450\n","Epoch 140/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5530 - accuracy: 0.4429\n","Epoch 141/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5594 - accuracy: 0.4454\n","Epoch 142/200\n","1563/1563 [==============================] - 60s 39ms/step - loss: 1.5528 - accuracy: 0.4439\n","Epoch 143/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5588 - accuracy: 0.4415\n","Epoch 144/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5523 - accuracy: 0.4448\n","Epoch 145/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5520 - accuracy: 0.4435\n","Epoch 146/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5532 - accuracy: 0.4437\n","Epoch 147/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5516 - accuracy: 0.4455\n","Epoch 148/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5460 - accuracy: 0.4467\n","Epoch 149/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5476 - accuracy: 0.4435\n","Epoch 150/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5473 - accuracy: 0.4460\n","Epoch 151/200\n","1563/1563 [==============================] - 60s 39ms/step - loss: 1.5441 - accuracy: 0.4481\n","Epoch 152/200\n","1563/1563 [==============================] - 62s 40ms/step - loss: 1.5430 - accuracy: 0.4493\n","Epoch 153/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5484 - accuracy: 0.4482\n","Epoch 154/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5372 - accuracy: 0.4500\n","Epoch 155/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5497 - accuracy: 0.4490\n","Epoch 156/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5401 - accuracy: 0.4497\n","Epoch 157/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5389 - accuracy: 0.4508\n","Epoch 158/200\n","1563/1563 [==============================] - 60s 39ms/step - loss: 1.5418 - accuracy: 0.4498\n","Epoch 159/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5453 - accuracy: 0.4505\n","Epoch 160/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5446 - accuracy: 0.4467\n","Epoch 161/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5613 - accuracy: 0.4433\n","Epoch 162/200\n","1563/1563 [==============================] - 62s 39ms/step - loss: 1.5417 - accuracy: 0.4477\n","Epoch 163/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5343 - accuracy: 0.4503\n","Epoch 164/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5658 - accuracy: 0.4432\n","Epoch 165/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5611 - accuracy: 0.4412\n","Epoch 166/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5692 - accuracy: 0.4392\n","Epoch 167/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5539 - accuracy: 0.4470\n","Epoch 168/200\n","1563/1563 [==============================] - 59s 37ms/step - loss: 1.5533 - accuracy: 0.4446\n","Epoch 169/200\n","1563/1563 [==============================] - 57s 36ms/step - loss: 1.5569 - accuracy: 0.4452\n","Epoch 170/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5549 - accuracy: 0.4434\n","Epoch 171/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5544 - accuracy: 0.4464\n","Epoch 172/200\n","1563/1563 [==============================] - 58s 37ms/step - loss: 1.5510 - accuracy: 0.4465\n","Epoch 173/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5471 - accuracy: 0.4491\n","Epoch 174/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5511 - accuracy: 0.4493\n","Epoch 175/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5562 - accuracy: 0.4425\n","Epoch 176/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5545 - accuracy: 0.4458\n","Epoch 177/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5430 - accuracy: 0.4481\n","Epoch 178/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5581 - accuracy: 0.4416\n","Epoch 179/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5528 - accuracy: 0.4455\n","Epoch 180/200\n","1563/1563 [==============================] - 62s 40ms/step - loss: 1.5484 - accuracy: 0.4473\n","Epoch 181/200\n","1563/1563 [==============================] - 63s 41ms/step - loss: 1.5566 - accuracy: 0.4430\n","Epoch 182/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5532 - accuracy: 0.4452\n","Epoch 183/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5450 - accuracy: 0.4491\n","Epoch 184/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5550 - accuracy: 0.4421\n","Epoch 185/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5428 - accuracy: 0.4480\n","Epoch 186/200\n","1563/1563 [==============================] - 63s 40ms/step - loss: 1.5423 - accuracy: 0.4496\n","Epoch 187/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5440 - accuracy: 0.4492\n","Epoch 188/200\n","1563/1563 [==============================] - 60s 39ms/step - loss: 1.5414 - accuracy: 0.4490\n","Epoch 189/200\n","1563/1563 [==============================] - 62s 40ms/step - loss: 1.5455 - accuracy: 0.4484\n","Epoch 190/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5403 - accuracy: 0.4477\n","Epoch 191/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5339 - accuracy: 0.4506\n","Epoch 192/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5385 - accuracy: 0.4522\n","Epoch 193/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5371 - accuracy: 0.4487\n","Epoch 194/200\n","1563/1563 [==============================] - 60s 38ms/step - loss: 1.5334 - accuracy: 0.4516\n","Epoch 195/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5373 - accuracy: 0.4541\n","Epoch 196/200\n","1563/1563 [==============================] - 63s 40ms/step - loss: 1.5353 - accuracy: 0.4533\n","Epoch 197/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5425 - accuracy: 0.4508\n","Epoch 198/200\n","1563/1563 [==============================] - 59s 38ms/step - loss: 1.5347 - accuracy: 0.4525\n","Epoch 199/200\n","1563/1563 [==============================] - 61s 39ms/step - loss: 1.5363 - accuracy: 0.4515\n","Epoch 200/200\n","1563/1563 [==============================] - 60s 39ms/step - loss: 1.5342 - accuracy: 0.4497\n","Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten_3 (Flatten)         (None, 3072)              0         \n","                                                                 \n"," dense_10 (Dense)            (None, 1024)              3146752   \n","                                                                 \n"," dropout_7 (Dropout)         (None, 1024)              0         \n","                                                                 \n"," dense_11 (Dense)            (None, 512)               524800    \n","                                                                 \n"," dropout_8 (Dropout)         (None, 512)               0         \n","                                                                 \n"," dense_12 (Dense)            (None, 512)               262656    \n","                                                                 \n"," dropout_9 (Dropout)         (None, 512)               0         \n","                                                                 \n"," dense_13 (Dense)            (None, 10)                5130      \n","                                                                 \n","=================================================================\n","Total params: 3,939,338\n","Trainable params: 3,939,338\n","Non-trainable params: 0\n","_________________________________________________________________\n","Evaluate on test data\n","313/313 [==============================] - 4s 11ms/step - loss: 1.5775 - accuracy: 0.4558\n","test loss, test acc: [1.5775424242019653, 0.45579999685287476]\n","Model saved to disk\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]}]}